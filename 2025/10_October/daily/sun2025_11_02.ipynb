{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-DotPZn3nJVU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문제 1 (주관식 - 개념 설명)\n",
        "- 경사 하강법(Gradient Descent)의 목표와 핵심 원리를 \"손실(Loss)\"과 \"경사(Gradient)\"라는 키워드를 사용하여 2~3줄로 간략히 설명하시오"
      ],
      "metadata": {
        "id": "REx3fnH2nZao"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "경사하강법은 손실함수를 편미분하여 손실을 최소화하는 머신러닝 알고리즘이다. 경사를 계산하여 가중치(파라메터)를 계속해서 업데이트하여 최적의 경사를 찾는 기능을 핵심으로 한다.\n",
        "\n"
      ],
      "metadata": {
        "id": "r0iPgNT8nrrj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "문제 2 (실습 문제 - 코드 작성)\n",
        "\n",
        "평균 제곱 오차(MSE: Mean Squared Error) 손실 함수를 파이토치 텐서를 사용하여 직접 구현하시오.\n",
        "\n",
        "- 함수 mse(Yp, Y)는 예측값 텐서 Yp와 실제값 텐서 Y를 입력받아 MSE 손실 값을 계산하여 반환해야 합니다.\n",
        "\n",
        "- MSE 공식: $\\text{loss} = \\frac{1}{n} \\sum_{i=1}^{n} (Yp_i - Y_i)^2$\n"
      ],
      "metadata": {
        "id": "LpaE01Y7nehv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# 아래 함수를 완성하시오\n",
        "def mse(Yp, Y):\n",
        "    # Yp: 예측값 텐서, Y: 실제값 텐서\n",
        "    loss = ((Yp - Y)**2).mean()\n",
        "    return loss\n",
        "\n",
        "# --- 테스트 코드 (수정 불필요) ---\n",
        "Yp_test = torch.tensor([1.0, 2.5, 3.8])\n",
        "Y_test  = torch.tensor([1.2, 2.0, 4.0])\n",
        "# 예상 MSE = ((1.0-1.2)^2 + (2.5-2.0)^2 + (3.8-4.0)^2) / 3 = (0.04 + 0.25 + 0.04) / 3 = 0.33 / 3 = 0.11\n",
        "test_loss = mse(Yp_test, Y_test)\n",
        "print(f\"테스트 MSE 손실: {test_loss:.4f}\")"
      ],
      "metadata": {
        "id": "kpDT0Xb9nWSz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a095ff2-71d1-43f3-ce76-16edd4301343"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "테스트 MSE 손실: 0.1100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문제 3 (실습 문제 - 코드 빈칸 채우기)\n",
        "\n",
        "아래 코드의 빈칸 ( # TODO: ... 부분)을 채워 파라미터 W와 B를 업데이트하는 과정을 완성하시오.\n",
        "\n",
        "요구사항\n",
        "\n",
        "1. loss.backward()를 호출하여 경사를 계산합니다.\n",
        "2. torch.no_grad() 컨텍스트 내에서 W와 B를 학습률(lr)과 계산된 경사(.grad)를 이용하여 업데이트합니다.\n",
        "3. 다음 반복을 위해 W와 B의 경사 값을 0으로 초기화합니다 (.grad.zero_()).\n",
        "\n",
        "- 참고 : first_ml.ipynb)의 경사 하강법 반복 학습 부분을 참고"
      ],
      "metadata": {
        "id": "iMr1g2wToDrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np # 예시 데이터 생성을 위해 사용\n",
        "\n",
        "# 예시 데이터 (수정 불필요)\n",
        "X = torch.tensor([-5.,  5.,  0.,  2., -2.]).float()\n",
        "Y = torch.tensor([-6.7, 10.3, -3.3, 5.0, -5.3]).float()\n",
        "\n",
        "# 초기 파라미터 및 학습률 (수정 불필요)\n",
        "W = torch.tensor(1.0, requires_grad=True).float()\n",
        "B = torch.tensor(1.0, requires_grad=True).float()\n",
        "lr = 0.001\n",
        "\n",
        "# 예측 함수 및 손실 함수 (수정 불필요)\n",
        "def pred(X): return W * X + B\n",
        "def mse(Yp, Y): return ((Yp - Y)**2).mean()\n",
        "\n",
        "# --- 1회 반복 학습 과정 ---\n",
        "# 예측 계산 (수정 불필요)\n",
        "Yp = pred(X)\n",
        "\n",
        "# 손실 계산 (수정 불필요)\n",
        "loss = mse(Yp, Y)\n",
        "\n",
        "# TODO: 1. 경사 계산\n",
        "loss.backward()\n",
        "\n",
        "# 경사 업데이트 (torch.no_grad() 사용)\n",
        "with torch.no_grad():\n",
        "    # TODO: 2. W 파라미터 업데이트\n",
        "    W -= lr * W.grad\n",
        "    # TODO: 3. B 파라미터 업데이트\n",
        "    B -= lr * W.grad\n",
        "\n",
        "# TODO: 4. W와 B의 경사 초기화\n",
        "W.grad.zero_()\n",
        "B.grad.zero_()\n",
        "\n",
        "# --- 결과 확인 (수정 불필요) ---\n",
        "print(f\"업데이트 후 W: {W.item():.4f}\") # 초기 W=1.0, 초기 loss=13.3520, W.grad=-19.04, lr=0.001 -> 1.0 - 0.001*(-19.04) = 1.01904\n",
        "print(f\"업데이트 후 B: {B.item():.4f}\") # 초기 B=1.0, B.grad=2.0, lr=0.001 -> 1.0 - 0.001*(2.0) = 0.998\n",
        "print(f\"W의 현재 경사: {W.grad}\") # 초기화 후에는 0 또는 None 이어야 함\n",
        "print(f\"B의 현재 경사: {B.grad}\") # 초기화 후에는 0 또는 None 이어야 함"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oWowWM-oOSD",
        "outputId": "035f1a30-4362-4e9d-e77d-6936e6b57f87"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "업데이트 후 W: 1.0190\n",
            "업데이트 후 B: 1.0190\n",
            "W의 현재 경사: 0.0\n",
            "B의 현재 경사: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문제 4 (실습 문제 - 코드 빈칸 채우기)\n",
        "- torch.optim 라이브러리를 사용하여 확률적 경사 하강법(SGD) 옵티마이저를 생성하고, 이를 이용해 파라미터를 업데이트하는 코드의 빈칸을 채우시오\n",
        "\n",
        "요구사항\n",
        "1. optim.SGD를 사용하여 W와 B를 업데이트하는 옵티마이저(optimizer)를 생성합니다. 학습률(lr)도 지정해야 합니다.\n",
        "2. 계산된 경사를 이용하여 파라미터를 업데이트하기 위해 optimizer.step()를 호출합니다.\n",
        "3. 다음 반복을 위해 옵티마이저에 연결된 파라미터들의 경사도를 0으로 초기화하기 위해 optimizer.zero_grad()를 호출합니다."
      ],
      "metadata": {
        "id": "9FoboM3hpPcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np # 예시 데이터 생성을 위해 사용\n",
        "\n",
        "# 예시 데이터 (수정 불필요)\n",
        "X = torch.tensor([-5.,  5.,  0.,  2., -2.]).float()\n",
        "Y = torch.tensor([-6.7, 10.3, -3.3, 5.0, -5.3]).float()\n",
        "\n",
        "# 초기 파라미터 및 학습률 (수정 불필요)\n",
        "W = torch.tensor(1.0, requires_grad=True).float()\n",
        "B = torch.tensor(1.0, requires_grad=True).float()\n",
        "lr = 0.001\n",
        "\n",
        "# 예측 함수 및 손실 함수 (수정 불필요)\n",
        "def pred(X): return W * X + B\n",
        "def mse(Yp, Y): return ((Yp - Y)**2).mean()\n",
        "\n",
        "# TODO: 1. SGD 옵티마이저 생성 (파라미터: [W, B], 학습률: lr)\n",
        "optimizer = optim.SGD([W,B], lr = lr)\n",
        "\n",
        "# --- 1회 반복 학습 과정 ---\n",
        "# 예측 계산 (수정 불필요)\n",
        "Yp = pred(X)\n",
        "\n",
        "# 손실 계산 (수정 불필요)\n",
        "loss = mse(Yp, Y)\n",
        "\n",
        "# 경사 계산 (수정 불필요)\n",
        "loss.backward()\n",
        "\n",
        "# TODO: 2. 옵티마이저를 이용한 파라미터 업데이트\n",
        "optimizer.step()\n",
        "\n",
        "# TODO: 3. 옵티마이저 경사 초기화\n",
        "optimizer.zero_grad()\n",
        "\n",
        "# --- 결과 확인 (수정 불필요) ---\n",
        "# 결과는 문제 3과 동일해야 함\n",
        "print(f\"옵티마이저 사용 후 W: {W.item():.4f}\")\n",
        "print(f\"옵티마이저 사용 후 B: {B.item():.4f}\")\n",
        "print(f\"W의 현재 경사: {W.grad}\")\n",
        "print(f\"B의 현재 경사: {B.grad}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7_68yY4n_ee",
        "outputId": "2212f90e-efbf-4bb7-ca37-a9cee7ec806e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "옵티마이저 사용 후 W: 1.0190\n",
            "옵티마이저 사용 후 B: 0.9980\n",
            "W의 현재 경사: None\n",
            "B의 현재 경사: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문제 5(주관식-개념설명)\n",
        "\n",
        "모델 학습 시 발생할 수 있는 과소적합(Underfitting)과 과대적합(Overfitting)의 특징을 편향(Bias)과 분산(Variance) 관점에서 각각 설명하시오. (각각 1~2줄)"
      ],
      "metadata": {
        "id": "ENK7dByipgd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "과소적합은 모델이 너무 단순해서 패턴을 충분히 학습하지 못하는 오류이다. 데이터가 편향될 때 과소적합이 발생한다.\n",
        "이와 반대로 과대적합은 모델이 너무 복잡해서 노이즈까지 학습하는 오류이다. 데이터가 분산될 때 과대적합이 발생한다."
      ],
      "metadata": {
        "id": "i8Rw3ACHprxU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "문제 6 (주관식 - 평가 지표 선택)\n",
        "- 다음과 같은 두 가지 상황에 가장 적합한 분류 모델 평가 지표를 각각 선택하고 그 이유를 간략히 설명하시오.\n",
        "\n",
        "- 상황 1: 환자의 의료 데이터를 분석하여 암 진단 여부를 예측하는 모델 (실제 암 환자를 놓치면 안 되는 경우, 즉 FN(False Negative)을 최소화해야 함)\n",
        "\n",
        "- 상황 2: 이메일 데이터를 분석하여 스팸 메일 여부를 필터링하는 모델 (정상 메일을 스팸으로 잘못 분류하면 안 되는 경우, 즉 FP(False Positive)를 최소화해야 함)\n",
        "\n",
        "- 선택 가능한 지표: 정확도(Accuracy), 정밀도(Precision), 재현율(Recall), F1-Score"
      ],
      "metadata": {
        "id": "JE9Iio5_rqGC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "상황1에 적합한 모델은 재현율(Recall)로 Recall = TP / (TP + FN)이다. 실제로\n",
        "상황2에 적합한 모델은 정밀도(Precision)로 Precision = TP / (TP + FP)이다. 정상메일을 스펨으로 분류하면 안되기 때문이다."
      ],
      "metadata": {
        "id": "2eqQ4XdWr1ol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 한 번에 설치\n",
        "!pip install torch numpy matplotlib seaborn scikit-learn"
      ],
      "metadata": {
        "id": "ToTU3essa6Uj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd835454-d4da-4d5d-939d-fb7191524331"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "4차시 실습 통합 실행파일\n",
        "모든 실습을 한 번에 실행할 수 있습니다\n",
        "\n",
        "Part 1: 기본설정 및 모델정의\n",
        "Part 2: 지도학습(분류와 회귀)\n",
        "Part 3: 비지도학습과 편향-분산\n",
        "Part 4: K-Fold 교차검증\n",
        "Part 5: 평가지표 계산\n",
        "Part 6: 전체 ML 파이프라인\n",
        "\n",
        "필수 라이브러리:\n",
        "pip install torch numpy matplotlib seaborn scikit-learn\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import make_classification, make_regression, make_blobs\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, roc_curve, mean_absolute_error, mean_squared_error, r2_score)\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# 시드 고정. 재현성\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# 한글 깨짐방지\n",
        "plt.rcParams[\"axes.unicode_minus\"] = False\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"4차시 실습: 인공지능 개론 - 통합실행\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# =====================================================================\n",
        "# Part 1: 모델 정의\n",
        "# =====================================================================\n",
        "print(\"\\n[Part 1] 모델 정의 중...\")\n",
        "\n",
        "class BinaryClassifier(nn.Module):\n",
        "    \"\"\"이진분류용 다층 퍼셉트론\"\"\"\n",
        "    \"\"\"클래스(0/1) 예측 \"\"\"\n",
        "    def __init__(self, input_dim):\n",
        "        super(BinaryClassifier, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_dim, 64)\n",
        "        self.layer2 = nn.Linear(64, 32)\n",
        "        self.layer3 = nn.Linear(32, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.sigmoid(self.layer3(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class Regressor(nn.Module):\n",
        "    \"\"\"회귀용 다층 퍼셉트론\"\"\"\n",
        "    \"\"\"회귀 MLP \"\"\"\n",
        "    def __init__(self, input_dim):\n",
        "        super(Regressor, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "print(\"모델 정의 완료\")\n",
        "\n",
        "# =====================================================================\n",
        "# Part 2: 지도학습\n",
        "# =====================================================================\n",
        "print(\"\\n[Part 2] 지도학습 - 분류와 회귀\")\n",
        "\n",
        "# 분류 데이터 생성 및 학습\n",
        "X_class, y_class = make_classification(\n",
        "    n_samples=1000, n_features=20, n_informative=15, n_redundant=5, weights=[0.7, 0.3], random_state=42\n",
        ")\n",
        "\n",
        "X_train_c, X_temp_c, y_train_c, y_temp_c = train_test_split(\n",
        "    X_class, y_class, test_size=0.4, random_state=42, stratify=y_class\n",
        ")\n",
        "X_val_c, X_test_c, y_val_c, y_test_c = train_test_split(\n",
        "    X_temp_c, y_temp_c, test_size=0.5, random_state=42, stratify=y_temp_c\n",
        ")\n",
        "\n",
        "# 표준화(정규화: 평균 0, 분산 1) 스케일링\n",
        "scaler_c = StandardScaler()\n",
        "X_train_c_scaled = scaler_c.fit_transform(X_train_c)\n",
        "X_val_c_scaled = scaler_c.transform(X_val_c)\n",
        "X_test_c_scaled = scaler_c.transform(X_test_c)\n",
        "\n",
        "# 스케일링 데이터를 텐서변환\n",
        "X_train_c_t = torch.FloatTensor(X_train_c_scaled)\n",
        "y_train_c_t = torch.FloatTensor(y_train_c).unsqueeze(1)\n",
        "X_val_c_t = torch.FloatTensor(X_val_c_scaled)\n",
        "y_val_c_t = torch.FloatTensor(y_val_c).unsqueeze(1)\n",
        "\n",
        "model_class = BinaryClassifier(input_dim=20)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model_class.parameters(), lr=0.001)\n",
        "\n",
        "print(\"분류 모델 학습 중...\")\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "patience_counter = 0\n",
        "best_model_state = None\n",
        "\n",
        "for epoch in range(100):\n",
        "    model_class.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model_class(X_train_c_t)\n",
        "    loss = criterion(outputs, y_train_c_t)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model_class.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model_class(X_val_c_t)\n",
        "        val_loss = criterion(val_outputs, y_val_c_t)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        best_model_state = model_class.state_dict()\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "\n",
        "    if patience_counter >= 10:\n",
        "        break\n",
        "\n",
        "model_class.load_state_dict(best_model_state)\n",
        "print(\"분류모델 학습완료\")\n",
        "\n",
        "# 회귀 데이터 생성 및 학습\n",
        "X_reg, y_reg = make_regression(\n",
        "    n_samples=800, n_features=10, n_informative=8, noise=10.0, random_state=42\n",
        ")\n",
        "\n",
        "X_train_r, X_test_r, y_train_r, y_test_r = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "scaler_r = StandardScaler()\n",
        "X_train_r_scaled = scaler_r.fit_transform(X_train_r)\n",
        "X_test_r_scaled = scaler_r.transform(X_test_r)\n",
        "\n",
        "X_train_r_t = torch.FloatTensor(X_train_r_scaled)\n",
        "y_train_r_t = torch.FloatTensor(y_train_r).unsqueeze(1)\n",
        "\n",
        "model_reg = Regressor(input_dim=10)\n",
        "criterion_reg = nn.MSELoss()\n",
        "optimizer_reg = optim.Adam(model_reg.parameters(), lr=0.01)\n",
        "\n",
        "print(\"회귀모델 학습 중...\")\n",
        "for epoch in range(100):\n",
        "    model_reg.train()\n",
        "    optimizer_reg.zero_grad()\n",
        "    outputs = model_reg(X_train_r_t)\n",
        "    loss = criterion_reg(outputs, y_train_r_t)\n",
        "    loss.backward()\n",
        "    optimizer_reg.step()\n",
        "\n",
        "print(\"회귀모델 학습완료\")\n",
        "\n",
        "# =====================================================================\n",
        "# Part 3: 비지도학습과 편향-분산\n",
        "# =====================================================================\n",
        "print(\"\\n[Part 3] 비지도학습과 편향-분산\")\n",
        "\n",
        "# K-Means 군집화\n",
        "X_cluster, y_true_cluster = make_blobs(\n",
        "    n_samples=300, centers=3, n_features=2, cluster_std=1.0, random_state=42\n",
        ")\n",
        "\n",
        "# kmeans\n",
        "kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n",
        "y_pred_cluster = kmeans.fit_predict(X_cluster)\n",
        "\n",
        "plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1,3,1)\n",
        "plt.scatter(\n",
        "    X_cluster[:,0], X_cluster[:, 0], c=y_true_cluster, cmap=\"viridis\", alpha=0.6, edgecolors=\"k\", s=50\n",
        "    )\n",
        "plt.title(\"True Labels\", fontsize=12)\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.colorbar()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.scatter(\n",
        "    X_cluster[:,0], X_cluster[:, 1], c=y_pred_cluster, cmap=\"plasma\", alpha=0.6, edgecolors=\"k\", s=50\n",
        "    )\n",
        "plt.scatter(\n",
        "    kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,0], c=\"red\", marker=\"X\", s=300, edgecolors=\"black\", linewidths=2\n",
        "            )\n",
        "plt.title(\"K-Means Result\", fontsize=12)\n",
        "plt.xlabel(\"Feature 1\")\n",
        "plt.ylabel(\"Feature 2\")\n",
        "plt.colorbar()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "cluster_counts = np.bincount(y_pred_cluster)\n",
        "plt.bar(\n",
        "    range(len(cluster_counts)), cluster_counts, color=[\"#440154\", \"#31688e\", \"#fde724\"], edgecolor=\"black\"\n",
        "    )\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"result_clustering.png\", dpi=150, bbox_inches=\"tight\")\n",
        "plt.close()\n",
        "print(\"저장: result_clustering.png\")\n",
        "\n",
        "# 편향-분산 트레이드오프\n",
        "np.random.seed(42)\n",
        "X_bias = np.sort(np.random.rand(100, 1) * 10, axis=0)\n",
        "y_bias = np.sin(X_bias).ravel() + np.random.randn(100) * 0.5\n",
        "X_test_bias = np.linspace(0, 10, 200).reshape(-1, 1)\n",
        "y_test_bias = np.sin(X_test_bias).ravel()\n",
        "\n",
        "degrees = [1, 3, 9, 20]\n",
        "colors = [\"#e74c3c\", \"#3498db\", \"#2ecc71\", \"#f39c12\"]\n",
        "\n",
        "plt.figure(figsize=(16, 4))\n",
        "\n",
        "for idx, degree in enumerate(degrees):\n",
        "    poly = PolynomialFeatures(degree=degree)\n",
        "    X_poly = poly.fit_transform(X_bias)\n",
        "    X_test_poly = poly.transform(X_test_bias)\n",
        "\n",
        "    model_bias = Ridge(alpha=0.01)\n",
        "    model_bias.fit(X_poly, y_bias)\n",
        "\n",
        "    train_pred = model_bias.predict(X_poly)\n",
        "    test_pred = model_bias.predict(X_test_poly)\n",
        "\n",
        "    train_mse = mean_squared_error(y_bias, train_pred)\n",
        "    test_mse = mean_squared_error(y_test_bias, test_pred)\n",
        "\n",
        "    plt.subplot(1, 4, idx +1)\n",
        "    plt.scatter(X_bias, y_bias, alpha=0.5, s=30, color=\"gray\", edgecolors=\"black\")\n",
        "    plt.plot(X_test_bias, y_test_bias, \"g--\", linewidth=2.5)\n",
        "    plt.plot(X_test_bias, test_pred, color=colors[idx], linewidth=2.5)\n",
        "    plt.title(f\"Degree {degree}\\nTrain: {train_mse: .3f} | Test: {test_mse: .3f}\")\n",
        "    plt.ylim(-2.5, 2.5)\n",
        "    plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"result_bias_variance.png\", dpi=150, bbox_inches=\"tight\")\n",
        "plt.close()\n",
        "print(\"저장: result_bias_variance.png\")\n",
        "\n",
        "# =====================================================================\n",
        "# Part 4: K-Fold 교차검증\n",
        "# =====================================================================\n",
        "print(\"\\n[Part 4] K-Fold 교차검증\")\n",
        "\n",
        "X_kfold, y_kfold = make_classification(\n",
        "    n_samples=500, n_features=20, n_informative=15, random_state=42\n",
        ")\n",
        "\n",
        "k = 5\n",
        "kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "fold_scores = []\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(kfold.split(X_kfold)):\n",
        "    X_train_fold = X_kfold[train_idx]\n",
        "    y_train_fold = y_kfold[train_idx]\n",
        "    X_val_fold = X_kfold[val_idx]\n",
        "    y_val_fold = y_kfold[val_idx]\n",
        "\n",
        "    scaler_fold = StandardScaler()\n",
        "    X_train_fold = scaler_fold.fit_transform(X_train_fold)\n",
        "    X_val_fold = scaler_fold.transform(X_val_fold)\n",
        "\n",
        "    X_train_fold_t = torch.FloatTensor(X_train_fold)\n",
        "    y_train_fold_t = torch.FloatTensor(y_train_fold).unsqueeze(1)\n",
        "    X_val_fold_t = torch.FloatTensor(X_val_fold)\n",
        "\n",
        "    model_fold = BinaryClassifier(input_dim=20)\n",
        "    optimizer_fold = optim.Adam(model_fold.parameters(), lr=0.01)\n",
        "    criterion_fold = nn.BCELoss()\n",
        "\n",
        "    for epoch in range(30):\n",
        "        model_fold.train()\n",
        "        optimizer_fold.zero_grad()\n",
        "        outputs = model_fold(X_train_fold_t)\n",
        "        loss = criterion_fold(outputs, y_train_fold_t)\n",
        "        loss.backward()\n",
        "        optimizer_fold.step()\n",
        "\n",
        "    model_fold.eval()\n",
        "    with torch.no_grad():\n",
        "        val_pred_prob = model_fold(X_val_fold_t).numpy().flatten()\n",
        "        val_pred = (val_pred_prob > 0.5).astype(int)\n",
        "\n",
        "    accuracy = accuracy_score(y_val_fold, val_pred)\n",
        "    fold_scores.append(accuracy)\n",
        "\n",
        "print(f\"K-Fold 평균 Accuracy: {np.mean(fold_scores): .4f} (std: {np.std(fold_scores): .4f})\")\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# Part 5: 평가 지표\n",
        "# =====================================================================\n",
        "print(\"\\n[Part 5] 평가지표 개선\")\n",
        "\n",
        "# 분류평가\n",
        "model_class.eval()\n",
        "X_test_c_t = torch.FloatTensor(X_test_c_scaled)\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_pred_prob_c = model_class(X_test_c_t).numpy().flatten()\n",
        "    y_pred_c = (y_pred_prob_c > 0.5).astype(int)\n",
        "\n",
        "cm = confusion_matrix(y_test_c, y_pred_c)\n",
        "accuracy = accuracy_score(y_test_c, y_pred_c)\n",
        "precision = precision_score(y_test_c, y_pred_c, zero_division=0)\n",
        "recall = recall_score(y_test_c, y_pred_c, zero_division=0)\n",
        "f1 = f1_score(y_test_c, y_pred_c, zero_division=0)\n",
        "auc = roc_auc_score(y_test_c, y_pred_prob_c)\n",
        "\n",
        "print(f\"분류 성능: Acc={accuracy: .3f}, Prec={precision: .3f}, Rec={recall: .3f}, F1={f1: .3f}, AUC={auc: .3f}\")\n",
        "\n",
        "fig = plt.figure(figsize=(15,5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap =\"Blues\", cbar=False,\n",
        "            xticklabels=[\"Pred 0\", \"Pred 1\"],\n",
        "            yticklabels=[\"True 0\", \"True 1\"]\n",
        "            )\n",
        "plt.title(\"Confusion Matrix\")\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "matrices = [\"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"AUC\"]\n",
        "values = [accuracy, precision, recall, f1, auc]\n",
        "plt.barh(matrices, values, color=[\"#3498db\", \"#2ecc71\", \"#e74c3c\", \"#f39c12\", \"#9b59b6\"])\n",
        "plt.xlim(0, 1.0)\n",
        "plt.title(\"Metrics\")\n",
        "plt.grid(axis=\"y\", alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "fpr, tpr, _ = roc_curve(y_test_c, y_pred_prob_c)\n",
        "plt.plot(fpr, tpr, linewidth=3, label=f\"AUC={auc: .3f}\")\n",
        "plt.plot([0, 1], [0, 1], \"k--\", linewidth=2)\n",
        "plt.xlabel(\"FPR\")\n",
        "plt.ylabel(\"TPR\")\n",
        "plt.title(\"ROC Curve\")\n",
        "plt.legend()\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"result_classification.png\", dpi=150, bbox_inches=\"tight\")\n",
        "plt.close()\n",
        "print(\"저장: result_classification.png\")\n",
        "\n",
        "# 회귀평가\n",
        "model_reg.eval()\n",
        "X_test_r_t = torch.FloatTensor(X_test_r_scaled)\n",
        "\n",
        "with torch.no_grad():\n",
        "    y_pred_reg = model_reg(X_test_r_t).numpy().flatten()\n",
        "\n",
        "mae = mean_absolute_error(y_test_r, y_pred_reg)\n",
        "mse = mean_squared_error(y_test_r, y_pred_reg)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test_r, y_pred_reg)\n",
        "\n",
        "print(f\"회귀성능: MAE={mae: .2f}, RMSE={rmse: .2f}, R2={r2: .3f}\")\n",
        "\n",
        "fig = plt.figure(figsize=(15, 5))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.scatter(y_test_r, y_pred_reg, alpha=0.6, s=50)\n",
        "plt.plot([y_test_r.min(), y_test_r.max()],\n",
        "         [y_test_r.min(), y_test_r.max()], \"r--\", linewidth=3)\n",
        "plt.xlabel(\"True\")\n",
        "plt.ylabel(\"predicted\")\n",
        "plt.title(\"prediction vs True\")\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "residuals = y_test_r - y_pred_reg\n",
        "\n",
        "plt.scatter(y_pred_reg, residuals, alpha=0.6, s=50)\n",
        "plt.axhline(y=0, color=\"r\", linestyle=\"--\", linewidth=3)\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residual Plot\")\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.hist(residuals, bins=30, color=\"skyblue\", edgecolor=\"black\", alpha=0.7)\n",
        "plt.axvline(x=0, color=\"red\", linestyle=\"--\", linewidth=3)\n",
        "plt.xlabel(\"Residuals\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution\")\n",
        "plt.grid(axis=\"y\", alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"result_regression.png\", dpi=150, bbox_inches=\"tight\")\n",
        "plt.close()\n",
        "print(\"저장: result_regression.png\")\n",
        "\n",
        "# =====================================================================\n",
        "# Part 6: ML 파이프라인\n",
        "# =====================================================================\n",
        "print(\"\\n[Part 6] ML 파이프라인 실행\")\n",
        "\n",
        "class MLPipeline:\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.best_score = 0\n",
        "        self.baseline_score = 0\n",
        "\n",
        "    def run(self, X, y):\n",
        "        print(\"\\nSTEP 1: 문제정의\")\n",
        "        print(\"목표: 고객 이탈 예측 (F1 > 0.80)\")\n",
        "\n",
        "        print(\"\\nSTEP 2: 데이터 준비\")\n",
        "        X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "            X, y, test_size=0.3, random_state=42, stratify=y\n",
        "        )\n",
        "        X_val, X_test, y_val, y_test = train_test_split(\n",
        "            X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        "        )\n",
        "\n",
        "        self.scaler = StandardScaler()\n",
        "        X_train = self.scaler.fit_transform(X_train)\n",
        "        X_val = self.scaler.transform(X_val)\n",
        "        X_test = self.scaler.transform(X_test)\n",
        "\n",
        "        print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
        "\n",
        "        print(\"\\nSTEP 3: 베이스라인\")\n",
        "        majority_class = np.bincount(y_train).argmax()\n",
        "        baseline_pred = np.full(len(y_test), majority_class)\n",
        "        self.baseline_score = f1_score(y_test, baseline_pred, zero_division=0)\n",
        "        print(f\"베이스라인 F1: {self.baseline_score: .4f}\")\n",
        "\n",
        "        print(\"\\nSTEP 4: 모델학습\")\n",
        "        self.model = BinaryClassifier(input_dim=X.shape[1])\n",
        "        criterion = nn.BCELoss()\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "\n",
        "        X_train_t = torch.FloatTensor(X_train)\n",
        "        y_train_t = torch.FloatTensor(y_train).unsqueeze(1)\n",
        "        X_val_t = torch.FloatTensor(X_val)\n",
        "        y_val_t = torch.FloatTensor(y_val).unsqueeze(1)\n",
        "\n",
        "        best_val_loss = float(\"inf\")\n",
        "        patience_counter = 0\n",
        "\n",
        "        for epoch in range(50):\n",
        "            self.model.train()\n",
        "            optimizer.zero_grad()\n",
        "            outputs = self.model(X_train_t)\n",
        "            loss = criterion(outputs, y_train_t)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            self.model.eval()\n",
        "            with torch.no_grad():\n",
        "                val_outputs = self.model(X_val_t)\n",
        "                val_loss = criterion(val_outputs, y_val_t)\n",
        "\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_model_state = self.model.state_dict()\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if patience_counter >= 10:\n",
        "                break\n",
        "\n",
        "        self.model.load_state_dict(best_model_state)\n",
        "        print(\"학습완료\")\n",
        "\n",
        "        print(\"\\nSTEP 5: 최종평가\")\n",
        "        self.model.eval()\n",
        "        X_test_t = torch.FloatTensor(X_test)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            y_pred_prob = self.model(X_test_t).numpy().flatten()\n",
        "            y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "        test_f1 = f1_score(y_test, y_pred, zero_division=0)\n",
        "        print(f\"테스트 F1: {test_f1: .4f}\")\n",
        "        print(f\"베이스라인 대비: {test_f1 - self.baseline_score: .4f}\")\n",
        "\n",
        "        if test_f1 > 0.80:\n",
        "            print(\"성공! 목표달성\")\n",
        "        else:\n",
        "            print(\"목표미달, 추가개선 필요\")\n",
        "\n",
        "X_proj, y_proj = make_classification(\n",
        "    n_samples=1000, n_features=20, n_informative=15, weights=[0.65, 0.35], random_state=42\n",
        ")\n",
        "\n",
        "pipeline = MLPipeline()\n",
        "pipeline.run(X_proj, y_proj)\n",
        "\n",
        "\n",
        "# =====================================================================\n",
        "# 최종 요약\n",
        "# =====================================================================\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"전체 실습 완료\")\n",
        "print(\"=\" * 70)\n",
        "print(\"\\n생성된 파일:\")\n",
        "print(\"  1. result_clustering.png     - 군집화 결과\")\n",
        "print(\"  2. result_bias_variance.png  - 편향-분산 트레이드오프\")\n",
        "print(\"  3. result_classification.png - 분류 평가\")\n",
        "print(\"  4. result_regression.png     - 회귀 평가\")\n",
        "print(\"\\n모든 실습이 정상적으로 완료되었습니다.\")\n",
        "print(\"=\" * 70)"
      ],
      "metadata": {
        "id": "bOuNh3WspAfL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd05c90d-14ac-4843-83b2-f371e0e4e7d8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "4차시 실습: 인공지능 개론 - 통합실행\n",
            "======================================================================\n",
            "\n",
            "[Part 1] 모델 정의 중...\n",
            "모델 정의 완료\n",
            "\n",
            "[Part 2] 지도학습 - 분류와 회귀\n",
            "분류 모델 학습 중...\n",
            "분류모델 학습완료\n",
            "회귀모델 학습 중...\n",
            "회귀모델 학습완료\n",
            "\n",
            "[Part 3] 비지도학습과 편향-분산\n",
            "저장: result_clustering.png\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/scipy/_lib/_util.py:1233: LinAlgWarning: Ill-conditioned matrix (rcond=2.35718e-21): result may not be accurate.\n",
            "  return f(*arrays, *other_args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "저장: result_bias_variance.png\n",
            "\n",
            "[Part 4] K-Fold 교차검증\n",
            "K-Fold 평균 Accuracy:  0.9320 (std:  0.0075)\n",
            "\n",
            "[Part 5] 평가지표 개선\n",
            "분류 성능: Acc= 0.880, Prec= 0.950, Rec= 0.633, F1= 0.760, AUC= 0.960\n",
            "저장: result_classification.png\n",
            "회귀성능: MAE= 12.76, RMSE= 15.62, R2= 0.991\n",
            "저장: result_regression.png\n",
            "\n",
            "[Part 6] ML 파이프라인 실행\n",
            "\n",
            "STEP 1: 문제정의\n",
            "목표: 고객 이탈 예측 (F1 > 0.80)\n",
            "\n",
            "STEP 2: 데이터 준비\n",
            "Train: 700, Val: 150, Test: 150\n",
            "\n",
            "STEP 3: 베이스라인\n",
            "베이스라인 F1:  0.0000\n",
            "\n",
            "STEP 4: 모델학습\n",
            "학습완료\n",
            "\n",
            "STEP 5: 최종평가\n",
            "테스트 F1:  0.7961\n",
            "베이스라인 대비:  0.7961\n",
            "목표미달, 추가개선 필요\n",
            "\n",
            "======================================================================\n",
            "전체 실습 완료\n",
            "======================================================================\n",
            "\n",
            "생성된 파일:\n",
            "  1. result_clustering.png     - 군집화 결과\n",
            "  2. result_bias_variance.png  - 편향-분산 트레이드오프\n",
            "  3. result_classification.png - 분류 평가\n",
            "  4. result_regression.png     - 회귀 평가\n",
            "\n",
            "모든 실습이 정상적으로 완료되었습니다.\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}